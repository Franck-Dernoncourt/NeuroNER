
#----- Possible modes of operation -----
# training mode (from scratch): set continue_training to True, and use_pretrained_model to False (if training from scratch). 
#				 				Must have train and valid sets in the dataset_text_folder, and test and deployment sets are optional.
# training mode (from pretrained model): set continue_training to True, and use_pretrained_model to True (if training from a pretrained model). 
#				 						 Must have train and valid sets in the dataset_text_folder, and test and deployment sets are optional.
# prediction mode (using pretrained model): set continue training to False, and use_pretrained_model to True. 
#											Must have either a test set or a deployment set.
# NOTE: Whenever use_pretrained_model is set to True, pretrained_model_checkpoint_filepath must be set to the pretrained model to use, and 
# 		dataset.pickle and parameters.ini must exist in the same folder as the checkpoint file.
#---------------------------------------

[mode]
# At least one of use_pretrained_model and train_model must be set to True.
train_model = True
use_pretrained_model = False
pretrained_model_checkpoint_filepath = ../trained_models/conll-2003-en/model_conll-2003-en.ckpt

[dataset]
dataset_text_folder = ../data/conll2003/en

# main_evaluation_mode should be either 'conll', 'bio', 'token', or 'binary'. ('conll' is entity-based)
# It determines which metric to use for early stopping, displaying during training, and plotting F1-score vs. epoch.
main_evaluation_mode = conll

[ann]
use_character_lstm = True
character_embedding_dimension = 25
character_lstm_hidden_state_dimension = 25

# In order to use random initialization instead, set token_pretrained_embedding_filepath to empty string, as below:
# token_pretrained_embedding_filepath =  
token_pretrained_embedding_filepath = ../data/word_vectors/glove.6B.100d.txt 
token_embedding_dimension = 100
token_lstm_hidden_state_dimension = 100

use_crf = True

[training]
patience = 10
maximum_number_of_epochs = 100

# optimizer should be either 'sgd', 'adam', or 'adadelta'
optimizer = sgd
learning_rate = 0.005

# dropout_rate should be between 0 and 1
dropout_rate = 0.5

# Upper bound on the number of CPU threads NeuroNER will use 
number_of_cpu_threads = 8

# Upper bound on the number of GPU NeuroNER will use 
# If number_of_gpus > 0, you need to have installed tensorflow-gpu
number_of_gpus = 0

[advanced]
experiment_name = test

# If remap_unknown_tokens is set to True, map to UNK any token that hasn't been seen in neither the training set nor the pre-trained token embeddings
remap_unknown_tokens_to_unk = True

# If debug is set to True, only 200 lines will be loaded for each split of the dataset.
debug = False
verbose = False

# plot_format specifies the format of the plots generated by NeuroNER. It should be either 'png' or 'pdf'.
plot_format = pdf